---
title: Architecture
description: Comprehensive guide to Shugur Relay's technical architecture, design patterns, and implementation details.
---

This document provides a detailed overview of Shugur Relay's technical architecture, design patterns, and implementation details.

## High-Level Architecture

Shugur Relay is designed as a **distributed relay cluster** that provides fault-tolerant, scalable Nostr infrastructure through a stateless, layered architecture. Clients connect directly to individual nodes in the cluster and handle failover themselves when nodes become unavailable.

```mermaid
graph TB
    subgraph "Client Layer"
        WB[Web Browser<br/>Dashboard]
        NC[Nostr Clients<br/>WebSocket]
        MT[Monitoring Tools<br/>Prometheus]
    end
    
    subgraph "Transport Layer"
        HS[HTTP Server<br/>NIP-11, API]
        WS[WebSocket<br/>Nostr Protocol]
        MS[Metrics Server<br/>Prometheus]
    end
    
    subgraph "Application Layer"
        WH[Web Handler]
        RS[Relay Server]
        NM[Node Manager]
        EV[Event Validation]
        CM[Connection Management]
        SM[Subscription Management]
        NI[NIPs Implementation]
        RL[Rate Limiting & Security]
        WP[Worker Pool Management]
    end
    
    subgraph "Storage Layer"
        DO[Database Operations]
        EP[Event Processor]
        BF[Bloom Filter<br/>Duplicate Detection]
    end
    
    subgraph "Persistence Layer"
        DB[(CockroachDB<br/>Distributed SQL Database)]
    end
    
    WB --> HS
    NC --> WS
    MT --> MS
    
    HS --> WH
    WS --> RS
    MS --> NM
    
    WH --> DO
    RS --> EV
    RS --> CM
    RS --> SM
    NM --> WP
    
    EV --> NI
    EV --> RL
    
    DO --> EP
    EP --> BF
    BF --> DB
    
    style DB fill:#e1f5fe
    style RS fill:#f3e5f5
    style NC fill:#e8f5e8
```

## Distributed Cluster Architecture

### Cluster Overview

Shugur Relay operates as a distributed cluster where multiple relay nodes work together to provide high availability and data redundancy. This architecture ensures that:

- **Direct Node Access**: Clients connect directly to individual nodes in the cluster
- **Client-Side Failover**: When a node becomes unavailable, clients reconnect to other healthy nodes
- **Data Replication**: Events are automatically replicated across cluster nodes using CockroachDB's distributed consensus
- **Transparent Scaling**: New nodes can be added to the cluster, and clients can discover and connect to them

```mermaid
graph TB
    subgraph "Client Connections"
        C1[Nostr Client 1]
        C2[Nostr Client 2]
        C3[Nostr Client 3]
    end
    
    subgraph "Shugur Distributed Cluster"
        subgraph "Relay Nodes"
            N1[Node 1]
            N2[Node 2]
            N3[Node 3]
        end
        
        subgraph "Distributed Database"
            DB1[(CockroachDB<br/>Node 1)]
            DB2[(CockroachDB<br/>Node 2)]
            DB3[(CockroachDB<br/>Node 3)]
        end
    end
    
    C1 --> N1
    C1 -.->|Failover| N2
    C1 -.->|Failover| N3
    
    C2 --> N2
    C2 -.->|Failover| N1
    C2 -.->|Failover| N3
    
    C3 --> N3
    C3 -.->|Failover| N1
    C3 -.->|Failover| N2
    
    N1 --> DB1
    N2 --> DB2
    N3 --> DB3
    
    DB1 -.->|Consensus Replication| DB2
    DB2 -.->|Consensus Replication| DB3
    DB3 -.->|Consensus Replication| DB1
    
    style N1 fill:#c8e6c9
    style N2 fill:#e1f5fe
    style N3 fill:#e1f5fe
    style DB1 fill:#f3e5f5
    style DB2 fill:#f3e5f5
    style DB3 fill:#f3e5f5
    style C1 fill:#e8f5e8
    style C2 fill:#e8f5e8
    style C3 fill:#e8f5e8
```

### Failover Behavior

1. **Health Monitoring**: Clients monitor connection health to their connected node
2. **Failure Detection**: Clients detect connection failures (network timeout, connection drop)
3. **Node Discovery**: Clients maintain a list of available cluster nodes
4. **Reconnection**: Clients reconnect to alternative nodes when failures occur
5. **Data Consistency**: All nodes serve the same data due to CockroachDB replication

### Benefits Over Traditional Multi-Relay Setup

| Traditional Nostr | Shugur Distributed Cluster |
|------------------|------------------|
| Client manages multiple connections | Client connects to cluster nodes |
| Manual failover logic required | Client handles failover between cluster nodes |
| Data scattered across relays | Unified data across cluster |
| Complex relay discovery | Cluster node discovery |
| Potential data inconsistency | Strong consistency via CockroachDB |

## Core Components

### 1. Application Node (`internal/application/`)

The central coordinator that manages all relay components.

**Responsibilities:**
- Component lifecycle management
- Dependency injection
- Configuration distribution
- Graceful shutdown coordination

**Key Files:**
- `node.go` - Main node implementation
- `node_builder.go` - Builder pattern for node construction
- `node_utils.go` - Utility functions

### 2. Relay Server (`internal/relay/`)

Handles the core Nostr protocol implementation.

**Responsibilities:**
- WebSocket connection management
- Nostr message processing
- Event validation and storage
- Subscription management

**Key Files:**
- `server.go` - HTTP/WebSocket server
- `connection.go` - WebSocket connection handling
- `subscription.go` - Subscription management
- `event_validator.go` - Event validation logic

### 3. Storage Layer (`internal/storage/`)

Manages data persistence and retrieval.

**Responsibilities:**
- Database connection management
- Event storage and retrieval
- Query optimization
- Bloom filter management

**Key Files:**
- `db.go` - Database connection and operations
- `event_processor.go` - Event processing pipeline
- `queries.go` - Database queries
- `schema.go` - Database schema management

### 4. NIPs Implementation (`internal/relay/nips/`)

Implements Nostr Improvement Proposals.

**Responsibilities:**
- Protocol compliance validation
- Event type specific processing
- Feature-specific logic

**Key Files:**
- `nip01.go` - Basic protocol flow
- `nip11.go` - Relay information document
- `nip17.go` - Private direct messages
- And many more NIP implementations...

## Design Patterns

### 1. Builder Pattern

Used for complex object construction, particularly the Node:

```go
type NodeBuilder struct {
    ctx     context.Context
    cfg     *config.Config
    privKey ed25519.PrivateKey
    // ... other fields
}

func (b *NodeBuilder) BuildDB() error { /* ... */ }
func (b *NodeBuilder) BuildWorkers() { /* ... */ }
func (b *NodeBuilder) Build() *Node { /* ... */ }
```

### 2. Interface Segregation

Components depend on interfaces rather than concrete implementations:

```go
type NodeInterface interface {
    RegisterConn(conn domain.WebSocketConnection)
    UnregisterConn(conn domain.WebSocketConnection)
    GetActiveConnectionCount() int64
    // ... other methods
}
```

### 3. Dependency Injection

Configuration and dependencies are injected through constructors:

```go
func NewServer(relayCfg config.RelayConfig, node domain.NodeInterface, fullCfg *config.Config) *Server
```

### 4. Worker Pool Pattern

Background task processing using worker pools:

```go
type WorkerPool struct {
    workers chan chan Job
    jobQueue chan Job
    quit chan bool
}
```

## Data Flow

### Event Processing Pipeline

```mermaid
sequenceDiagram
    participant C as Client (WebSocket)
    participant R as Relay Server
    participant D as Database (CockroachDB)
    
    C->>R: ["EVENT", event]
    
    Note over R: 1. Parse JSON<br/>2. Validate Signature<br/>3. Check NIPs<br/>4. Rate Limit<br/>5. Policy Check
    
    R->>D: Store Event
    D-->>R: Success/Error
    R-->>C: ["OK", id, bool, message]
    
    Note over R: Notify Subscribers
```

### Subscription Flow

```mermaid
sequenceDiagram
    participant C as Client (WebSocket)
    participant R as Relay Server  
    participant D as Database (CockroachDB)
    
    C->>R: ["REQ", id, filters...]
    
    Note over R: 1. Validate Filters<br/>2. Check Limits<br/>3. Store Subscription
    
    R->>D: Query Events
    D-->>R: Event Results
    
    loop For each matching event
        R-->>C: ["EVENT", id, event]
    end
    
    R-->>C: ["EOSE", id]
```

## Concurrency Model

### Goroutine Usage

1. **Main Server Goroutine** - HTTP server listener
2. **Connection Goroutines** - One per WebSocket connection
3. **Worker Pool Goroutines** - Background task processing
4. **Metrics Goroutines** - Metrics collection and export
5. **Database Pool Goroutines** - Connection pool management

### Synchronization Mechanisms

#### 1. Mutexes

Used for protecting shared state:

```go
type WsConnection struct {
    subscriptionsMu sync.RWMutex
    subscriptions   map[string]SubscriptionInfo
    // ...
}
```

#### 2. Channels

Used for communication between goroutines:

```go
type WorkerPool struct {
    jobQueue chan Job
    workers  chan chan Job
    quit     chan bool
}
```

#### 3. Atomic Operations

Used for counters and flags:

```go
type WsConnection struct {
    isClosed atomic.Bool
    // ...
}
```

## Storage Architecture

### Database Schema

```sql
-- Events table (main storage)
CREATE TABLE events (
    id TEXT PRIMARY KEY,
    pubkey TEXT NOT NULL,
    created_at BIGINT NOT NULL,
    kind INTEGER NOT NULL,
    tags JSONB,
    content TEXT,
    sig TEXT NOT NULL,
    
    INDEX idx_events_pubkey (pubkey),
    INDEX idx_events_kind (kind),
    INDEX idx_events_created_at (created_at),
    INDEX idx_events_tags (tags)
);

-- Additional indexes for performance
CREATE INDEX idx_events_composite ON events (kind, created_at);
CREATE INDEX idx_events_search ON events USING gin(to_tsvector('english', content));
```

### Query Optimization

#### 1. Bloom Filters

Used for quick duplicate detection:

```go
type DB struct {
    Bloom *bloom.BloomFilter
    // ...
}

// Check if event might exist
if !db.Bloom.TestString(eventID) {
    // Definitely new event
    return false
}
```

#### 2. Connection Pooling

Optimized database connection management:

```go
pool, err := pgxpool.New(ctx, dbURI)
```

#### 3. Prepared Statements

Cached query plans for better performance.

## Security Architecture

### Input Validation

#### 1. Event Validation Pipeline

```go
func (ev *EventValidator) ValidateEvent(event *nostr.Event) error {
    // 1. Structural validation
    if err := ev.validateStructure(event); err != nil {
        return err
    }
    
    // 2. Signature verification
    if err := ev.validateSignature(event); err != nil {
        return err
    }
    
    // 3. NIP-specific validation
    if err := ev.validateWithNIPs(event); err != nil {
        return err
    }
    
    return nil
}
```

#### 2. Rate Limiting

Token bucket algorithm implementation:

```go
type RateLimiter struct {
    limiters sync.Map // map[string]*rate.Limiter
}

func (rl *RateLimiter) Allow(key string) bool {
    limiter := rl.getLimiter(key)
    return limiter.Allow()
}
```

### Access Control

#### 1. Blacklist/Whitelist System

```go
type Node struct {
    blacklistPubKeys map[string]struct{}
    whitelistPubKeys map[string]struct{}
}
```

#### 2. Progressive Banning

Escalating penalties for policy violations.

## Performance Optimizations

### Memory Management

#### 1. Object Pooling

Reuse of frequently allocated objects:

```go
var eventPool = sync.Pool{
    New: func() interface{} {
        return &nostr.Event{}
    },
}
```

#### 2. Bloom Filter Usage

Reduces database queries for duplicate detection.

#### 3. Connection Caching

WebSocket connections are kept alive and reused.

### CPU Optimization

#### 1. Goroutine Pool

Limited number of worker goroutines to prevent resource exhaustion.

#### 2. Efficient JSON Processing

Optimized JSON parsing and generation.

#### 3. Signature Verification Caching

Cache results of expensive cryptographic operations.

## Deployment Models

### Standalone Deployment

```mermaid
graph TB
    C[Clients] <--> R[Single Shugur Relay Node]
    R <--> DB[(Single CockroachDB Instance)]
    
    style R fill:#f3e5f5
    style DB fill:#e1f5fe
    style C fill:#e8f5e8
```

- **Pros**: Simple to set up and manage
- **Cons**: Not highly available; single point of failure

### Distributed Deployment

```mermaid
graph TB
    C[Clients] --> R1[Relay Node 1]
    C -.->|Failover| R2[Relay Node 2]
    C -.->|Failover| R3[Relay Node 3]
    
    R1 <--> DB1[(CockroachDB Node 1)]
    R2 <--> DB2[(CockroachDB Node 2)]
    R3 <--> DB3[(CockroachDB Node 3)]
    
    DB1 -.-> DB2
    DB2 -.-> DB3
    DB3 -.-> DB1
    
    style R1 fill:#c8e6c9
    style R2 fill:#e1f5fe
    style R3 fill:#e1f5fe
    style DB1 fill:#f3e5f5
    style DB2 fill:#f3e5f5
    style DB3 fill:#f3e5f5
    style C fill:#e8f5e8
```

- **Pros**: Highly available and horizontally scalable; tolerant to node failures; clients handle load distribution
- **Cons**: Clients need to implement failover logic; more complex client setup

## Monitoring and Observability

### Metrics Collection

#### 1. Prometheus Metrics

```go
var (
    EventsReceived = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "relay_events_received_total",
            Help: "Total number of events received",
        },
        []string{"result"},
    )
)
```

#### 2. Structured Logging

```go
logger.Info("Event processed",
    zap.String("event_id", event.ID),
    zap.String("pubkey", event.PubKey),
    zap.Int("kind", event.Kind),
)
```

### Health Checks

#### 1. Database Health

Regular database connectivity checks.

#### 2. Memory Usage Monitoring

Track memory usage and garbage collection.

#### 3. Connection Metrics

Monitor active connections and connection rates.

## Error Handling

### Error Classification

#### 1. Validation Errors

Errors in event format or content:

```go
type ValidationError struct {
    Field   string
    Message string
}
```

#### 2. System Errors

Infrastructure or runtime errors:

```go
type SystemError struct {
    Component string
    Cause     error
}
```

### Error Recovery

#### 1. Circuit Breaker Pattern

Automatic fallback when dependencies fail.

#### 2. Graceful Degradation

Continue operating with reduced functionality.

#### 3. Retry Logic

Exponential backoff for transient failures.

## Scalability Considerations

### Horizontal Scaling

#### 1. Stateless Design

The relay can be horizontally scaled by running multiple instances.

#### 2. Database Clustering

CockroachDB provides built-in clustering capabilities.

#### 3. Load Balancing

WebSocket connections can be load balanced across instances.

### Vertical Scaling

#### 1. Resource Optimization

Efficient use of CPU, memory, and network resources.

#### 2. Connection Limits

Configurable limits based on available resources.

#### 3. Database Tuning

Optimized database configuration for the workload.

## Extension Points

### Plugin System (Future)

Architecture supports future plugin development:

```go
type Plugin interface {
    Init(ctx context.Context, cfg PluginConfig) error
    ProcessEvent(event *nostr.Event) error
    Shutdown() error
}
```

### Custom NIPs

New NIP implementations can be added easily:

```go
func RegisterNIP(kind int, validator func(*nostr.Event) error) {
    nipValidators[kind] = validator
}
```

### Custom Storage Backends

Storage layer can be extended with different backends.

## Related Documentation

- **[NIPs Support](/nips-support)**: Comprehensive NIP implementation details
- **[Configuration](/configuration)**: Configure your relay settings
- **[Performance Guide](/performance)**: Optimize for production workloads
- **[API Reference](/api)**: WebSocket and HTTP endpoint documentation
- **[Installation Guide](/installation)**: Choose your deployment method
